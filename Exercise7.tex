% !TeX spellcheck = da_DK
\documentclass[11pt,a4paper,oneside]{article}

% Overordnet opsætning
\setlength\parindent{24pt}
\setlength\parskip{3pt}
\setlength{\headheight}{14pt}
\renewcommand{\baselinestretch}{1.5}

\usepackage[utf8]{inputenc}
\usepackage[danish,english]{babel}
\usepackage{amsfonts,amsmath} %math: align og gather mm.; fonts: flere forskellige symboler
\usepackage[left=2cm, right=2cm, bottom=2cm]{geometry}
\usepackage{enumitem} %anvendes til Ã¦ndring i itemize mm.
\usepackage{fancyhdr} %Opsaening af sidehoved og -fod
\usepackage{lastpage} % Side "x af X".
\usepackage[ocgcolorlinks,linkcolor=black,urlcolor=black]{hyperref}
\usepackage{indentfirst} %"Indent" efter section og chapters etc.
\usepackage{graphicx} % for plotting graphs in matrix
\usepackage{subcaption} % figure "footnotes"

% bibliography
% \usepackage{csquotes}
% \usepackage{biblatex}
% \addbibresource{bibliography.bib}

% Ændring af titler
\usepackage{sectsty}
\subsectionfont{\normalfont\bfseries}
\subsubsectionfont{\normalfont\itshape\underline}

%Rstudio pakker
\usepackage[svgnames]{xcolor}
\usepackage{listings}

\lstset{language=R,
    basicstyle=\small\ttfamily,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
}

% fancy table
\usepackage{booktabs}
\usepackage{multirow}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\rightmark}
\lfoot{\author}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

%Table of Contents (toc) 
\setcounter{secnumdepth}{0}
\setcounter{tocdepth}{1}

%symbolforkortelser
\newcommand{\lll}{\mathcal{L}}
\newcommand{\LL}{\Leftrightarrow}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\rb}{\right]}
\newcommand{\lb}{\left[}
\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\cc}{\mathbb{C}}
\newcommand{\ee}{\mathbf{E}}
\newcommand{\ff}{\mathcal{F}}
\newcommand{\for}{\text{ for }}
\newcommand{\ggg}{\mathcal{G}}
\newcommand{\hh}{\mathcal{H}}
\newcommand{\pp}{\mathcal{P}}
\newcommand{\qq}{\mathcal{Q}}
\newcommand{\vv}{\mathbf{V}}
\newcommand{\rr}{\mathbf{R}}
\newcommand{\nn}{\mathbf{N}}
\newcommand{\nnn}{\mathcal{N}}
\newcommand{\ii}{\mathbf{1}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\bs}{\text{BS}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\sumt}{\sum_{t=1}^T}
\newcommand{\var}{\text{VaR}_{t,1}^\alpha}
\DeclareMathOperator*{\argmin}{\arg\,\min}


\selectlanguage{english}

% Sidetal er romertal frem til første kapitel
\pagenumbering{roman}

\lstset{language=R}

% Environment for theorems etc.
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{proof}{Proof}

\title{Exercise sheet 7}
\author{William Gram}
\date{\today}

\begin{document}
\maketitle

\pagenumbering{arabic}
\rfoot{\thepage}
\lfoot{Code at (click): \href{https://github.com/WiGram/fe_python}{GitHub/WiGram}}

\section{Exercise 1}
\renewcommand{\theequation}{1.\arabic{equation}}
\setcounter{equation}{0}

\subsection{1.1}
Show that with two random variables $X$ and $Y$, where $\ee\lb Y \vert X\rb$ is well-defined, one can always write:
\begin{align}
    Y = \ee\lb Y \vert X\rb + e,
\end{align}

where the error $e$ is uncorrelated with the "regressor" $X$, that is $\ee\lb e X\rb = 0$.

We have that:
\begin{gather*}
    \ee\lb x X\rb = \ee\lb \lp Y - \ee\lb Y\vert X\rb \rp X\rb = \ee\lb Y X - \ee\lb Y\vert X\rb X \rb = 0\\
    \LL \ee\lb Y X\rb - \ee\lb \ee\lb Y\vert X\rb X\rb = \ee\lb YX \rb - \ee\lb \ee\lb \ee\lb Y \vert X\rb \rb X\rb = \ee\lb YX\rb - \ee\lb YX\rb = 0.
\end{gather*}

The intuition: We may write a random variable $Y$ as its conditional expectation given $X$ plus some noise $e$ where the noise contains everything not explained by $X$.

\subsection{1.2}
If $X$ and $Y$ at Gaussian, then:
\begin{align}
    \lp X, Y\rp' \overset{D}{=} \nnn_2\lp \mu, \Omega\rp,
\end{align}
where
\begin{align}
    \mu = \begin{pmatrix} \mu_X \\ \mu_Y\end{pmatrix}, \quad
    \Omega = 
        \begin{pmatrix}
            \sigma_X^2 & \sigma_{XY} \\
            \sigma_{YX}& \sigma_Y^2
        \end{pmatrix}.
\end{align}

Show that we can write (1.1) as:
\begin{align}
    Y = \beta_1 + \beta_2 X + \varepsilon,
\end{align}
where $\varepsilon \iid \nnn\lp 0, \sigma^2\rp$.

First, we use that the following expectation holds by definition:
\begin{align}
    \ee\lb Y\vert X\rb = \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}\lp X - \mu_X\rp.
\end{align}

Inserting (1.5) into (1.1) we find that:
\begin{align}
    Y 
        &= \ee\lb Y\vert X\rb = \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}\lp X - \mu_X\rp + e \notag \\
        &= \lp \mu_Y - \frac{\sigma_{XY}}{\sigma_X^2} \mu_X\rp + \frac{\sigma_{XY}}{\sigma_X^2}X + e\notag\\
        &= \beta_1 + \beta_2 X + \varepsilon,
\end{align}
where $\beta_1 =\lp \mu_Y - \frac{\sigma_{XY}}{\sigma_X^2} \mu_X\rp$, and $\beta_2 = \frac{\sigma_{XY}}{\sigma_X^2}$, as well as $e = \varepsilon$.

The result shows that we have derived OLS by assuming the variables are normal. It also implicates, that OLS could not be derived if the variables are not Gaussian.

\subsection{1.3}
With $\lc y_t\rb_{t\geq 0}$ an AR(1) process with $\vert \rho\vert < 1$ and $\varepsilon_t \iid\nnn\lp 0, \sigma^2\rp$, we can write:
\begin{align}
    y_t = \rho y_{t-1} + \varepsilon_t.
\end{align}

From (1.7) we know that $y_t$ has a stationary representation, given by:
\begin{align}
    y_t = \sum_{i=0}^\infty \rho^i \varepsilon_{t-i}.
\end{align}

We want to show, that this can be written in vector-form:
\begin{align}
    \begin{pmatrix} y_t \\ y_{t-1}\end{pmatrix} \overset{D}{=} \nnn_2\lp
        \begin{pmatrix}0 \\ 0\end{pmatrix},
        \begin{pmatrix}
            \frac{\sigma^2}{1 - \rho^2} & \frac{\rho \sigma^2}{1 - \rho^2}\\
            \frac{\rho \sigma^2}{1 - \rho^2} & \frac{\sigma^2}{1 - \rho^2}
        \end{pmatrix}
        \rp.
\end{align}

We start by deriving $y_{t-1}$:
\begin{align}
    y_{t-1} = \sum_{i=0}^\infty \rho^i \varepsilon_{\lp t - 1\rp - i},
\end{align}
which follows from (1.8). Next, we have that:
\begin{align}
    y_t 
        = \sum_{i=0}^\infty \rho^i \varepsilon_{t-i}
        = \rho^0 \varepsilon_{t-0} + \sum_{i = 1}^\infty \rho^{i}\varepsilon_{t - i}
        = \varepsilon_t + \sum_{i = 0}^\infty\rho^{i + 1}\varepsilon_{t - \lp i + 1\rp}
        = \varepsilon_t + \rho\sum_{i=0}^\infty\rho^i\varepsilon_{t-1-i}.
\end{align}

This shows that:
\begin{align}
    \begin{pmatrix} y_t \\ y_{t-1} \end{pmatrix} 
    =
        \begin{pmatrix}
            \varepsilon_t + \rho\sum_{i=0}^\infty\rho^i\varepsilon_{t-1-i} \\
            \sum_{i=0}^\infty \rho^i \varepsilon_{\lp t - 1\rp - i}
        \end{pmatrix}.
\end{align}

Since all shocks $\varepsilon$ are iid with mean zero, $\ee\lb Y\rb = \lp 0, 0\rp'$. The covariance matrix of $y_{t-i}$ for $i \in 0, \dots, t$ is given by:
\begin{align}
    \vv\lp y_t\rp 
        &=\vv\lp \sum_{i=0}^t \rho^i \varepsilon_{t-i}\rp 
        = \sum_{i=0}^t \rho^{2i}\vv\lp \varepsilon_{t-i}\rp 
        = \sigma^2\sum_{i=0}^t \rho^{2i} \rightarrow \frac{\sigma^2}{1 - \rho^2}, \quad t\rightarrow \infty, \\
    Cov\lp y_t, y_{t-1}\rp 
        &= Cov\lp \varepsilon_t + \rho\sum_{i=0}^t \rho^i\varepsilon_{t-1-i}, \sum_{i=0}^t \rho^i\varepsilon_{t-1-i}\rp \notag \\ 
        &= \rho \sum_{i=0}^t \rho^{2i}\vv\lp \varepsilon_{t-1-i}\rp
        = \rho \sigma^2\sum_{i=0}^t\rho^{2i} \rightarrow \frac{\rho\sigma^2}{1 - \rho^2}, \quad t \rightarrow \infty \\
    \vv\lp y_{t-1}\rp
        &=\vv\lp \sum_{i=0}^\infty \rho^i \varepsilon_{t-1-i}\rp \rightarrow \frac{\sigma^2}{1 - \rho^2}, \quad t \rightarrow \infty.
\end{align}

This shows that the vector $\lp y_t, y_{t-1}\rp$ has the distribution given in (1.9).

\end{document}